{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Information From Personal Google Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = 'Datasets/Google-20-09-20/My Activity/Search/MyActivity.html'\n",
    "\n",
    "OUTPUT_FILE = 'google_data'\n",
    "OUTPUT_TYPE = 'csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Google Data HTML To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: [DONE]\n",
      "Parsing HTML file: [DONE]\n",
      "Successfully processed row: 0\n",
      "Successfully processed row: 5000\n",
      "Successfully processed row: 10000\n",
      "Successfully processed row: 15000\n",
      "Successfully processed row: 20000\n",
      "Successfully processed row: 25000\n",
      "Successfully processed row: 30000\n",
      "Successfully processed row: 35000\n",
      "Successfully processed row: 40000\n",
      "Successfully processed row: 45000\n",
      "Successfully processed row: 50000\n",
      "Successfully processed row: 55000\n",
      "Successfully processed row: 60000\n",
      "Successfully processed row: 65000\n",
      "Successfully processed row: 70000\n",
      "Successfully processed row: 75000\n",
      "Successfully processed row: 80000\n",
      "Successfully processed row: 85000\n",
      "Successfully processed row: 90000\n",
      "Successfully processed row: 95000\n",
      "Successfully processed row: 100000\n",
      "Successfully processed row: 105000\n",
      "Successfully processed row: 110000\n",
      "Successfully processed row: 115000\n",
      "Finished processing rows\n",
      "Saved to:  google_data.csv\n"
     ]
    }
   ],
   "source": [
    "class ProcessGoogleData:\n",
    "    ''' Handles Google Data '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.html = None\n",
    "        self.cells = None\n",
    "        self.df = None\n",
    "        self.file_name = None\n",
    " \n",
    "    def read_file(self, path):\n",
    "        print('Reading file: ', end='')\n",
    "        with open(path, 'r') as f:\n",
    "            self.html = f.read()    \n",
    "        print('[DONE]')\n",
    "            \n",
    "    def parse(self):\n",
    "        \"\"\" Input HTML data and parse through cells. \"\"\"\n",
    "        print('Parsing HTML file: ', end='')\n",
    "        soup = BeautifulSoup(self.html, 'lxml')\n",
    "        self.cells = soup.select('.content-cell')\n",
    "        print('[DONE]')\n",
    "    \n",
    "    def _process_row(self, row):\n",
    "        a = row.find('a')             \n",
    "        row_dict = {}\n",
    "        \n",
    "        try:\n",
    "            search_query = str(a.text)\n",
    "            search_type = str(a.parent.contents[0]).strip()\n",
    "            location = self._get_location(a)\n",
    "\n",
    "            # Handle different string format for Visited pages\n",
    "            if search_type == 'Visited':\n",
    "                site_url = a.get('href').split('?q=')[1]\n",
    "            else:\n",
    "                site_url = a.get('href')\n",
    "\n",
    "            search_date = pd.to_datetime(a.parent.contents[3])\n",
    "\n",
    "            # Build the dictionary\n",
    "            row_dict = {\n",
    "                'Type':search_type,\n",
    "                'Query': search_query,\n",
    "                'Date': search_date,\n",
    "                'URL': site_url,\n",
    "                'Location': location\n",
    "            }\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        return row_dict\n",
    "    \n",
    "    def _get_location(self, url):\n",
    "        \"\"\" Given a list of cells, find the location URL. \"\"\"\n",
    "        href_next = url.find_next('a').get('href')\n",
    "        \n",
    "        if 'google.com/maps/@?api=1' in href_next:\n",
    "            coordinates = href_next.split('center=')[1].split('&zoom')[0]\n",
    "            full_maps_url = 'https://www.google.com/maps/@' + coordinates + ',17z' \n",
    "            return full_maps_url\n",
    "        return None\n",
    "    \n",
    "    def create_table(self):\n",
    "        \"\"\" Takes a BS4 element filter, and appends each Google data cell into a dataframe row. \"\"\"        \n",
    "        meta_dict = {}\n",
    "        \n",
    "        cell_list = self.cells\n",
    "        \n",
    "        for i, elem in enumerate(cell_list):\n",
    "#             if i == 10:\n",
    "#                 break\n",
    "            row = self._process_row(elem)\n",
    "            meta_dict[i] = row\n",
    "\n",
    "            # Files are large, update status every 5000 elements\n",
    "            if (i % 5000 == 0):\n",
    "                print(f'Successfully processed HTML element: {i}')\n",
    "\n",
    "        df = pd.DataFrame.from_dict(data=meta_dict, orient='index')\n",
    "        print('Finished processing rows')\n",
    "        self.df = df\n",
    "        \n",
    "    def save_file(self, file_name='google_data', file_type='csv'):\n",
    "        file = file_name + '.' + file_type.lower()\n",
    "        self.file_name = file\n",
    "        \n",
    "        if file_type.lower() == 'csv':\n",
    "            self.df.to_csv(file)\n",
    "        if file_type.lower() == \"xlsx\":\n",
    "            self.df.to_excel(file)\n",
    "        \n",
    "        print(f'Saved {df.shape[0]} rows to: \"{file}\"')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    gd = ProcessGoogleData()\n",
    "    gd.read_file(path)\n",
    "    gd.parse()\n",
    "    gd.create_table()\n",
    "    \n",
    "    # Save to CSV\n",
    "    gd.save_file(file_name=OUTPUT_FILE, file_type=OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build New Features From Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Query</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Location</th>\n",
       "      <th>Day</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>Site</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>ayurveda dataset</td>\n",
       "      <td>2020-09-19 05:26:40+00:00</td>\n",
       "      <td>https://www.google.com/search?q=ayurveda+dataset</td>\n",
       "      <td>https://www.google.com/maps/@40.723780,-73.971...</td>\n",
       "      <td>2020-09-19</td>\n",
       "      <td>False</td>\n",
       "      <td>Google</td>\n",
       "      <td>East River, New York County, New York, United ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Visited</td>\n",
       "      <td>How Data Mining is useful in Ayurveda - Journa...</td>\n",
       "      <td>2020-09-19 05:17:48+00:00</td>\n",
       "      <td>http://www.ayurvedjournal.com/JAHM_201623_01.p...</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-09-19</td>\n",
       "      <td>False</td>\n",
       "      <td>Ayurvedjournal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Visited</td>\n",
       "      <td>Graph Algorithms: Practical Examples in Apache...</td>\n",
       "      <td>2020-09-09 03:11:32+00:00</td>\n",
       "      <td>https://www.goodreads.com/book/show/42832585-g...</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>False</td>\n",
       "      <td>Goodreads</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>Graph Algorithms: Practical Examples in Apache...</td>\n",
       "      <td>2020-09-09 03:11:23+00:00</td>\n",
       "      <td>https://www.google.com/search?q=Graph+Algorith...</td>\n",
       "      <td>https://www.google.com/maps/@40.723780,-73.971...</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>False</td>\n",
       "      <td>Google</td>\n",
       "      <td>East River, New York County, New York, United ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>graph analytics data science book</td>\n",
       "      <td>2020-09-09 03:10:53+00:00</td>\n",
       "      <td>https://www.google.com/search?q=graph+analytic...</td>\n",
       "      <td>https://www.google.com/maps/@40.723780,-73.971...</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>False</td>\n",
       "      <td>Google</td>\n",
       "      <td>East River, New York County, New York, United ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116052</th>\n",
       "      <td>Visited</td>\n",
       "      <td>http://www.stack.com/2013/11/11/at-home-should...</td>\n",
       "      <td>2015-08-17 01:21:25+00:00</td>\n",
       "      <td>http://www.stack.com/2013/11/11/at-home-should...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>Stack</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116055</th>\n",
       "      <td>Visited</td>\n",
       "      <td>http://ashotofadrenaline.net/20-hardest-should...</td>\n",
       "      <td>2015-08-17 01:15:51+00:00</td>\n",
       "      <td>http://ashotofadrenaline.net/20-hardest-should...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>Ashotofadrenaline</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116058</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>no weight shoulder exercises</td>\n",
       "      <td>2015-08-17 01:15:40+00:00</td>\n",
       "      <td>https://www.google.com/search?q=no+weight+shou...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>Google</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116061</th>\n",
       "      <td>Visited</td>\n",
       "      <td>http://www.mensfitness.com/training/workout-ro...</td>\n",
       "      <td>2015-08-17 01:11:06+00:00</td>\n",
       "      <td>http://www.mensfitness.com/training/workout-ro...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>Mensfitness</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116064</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>at home shoulder workouts no weights</td>\n",
       "      <td>2015-08-17 01:11:02+00:00</td>\n",
       "      <td>https://www.google.com/search?q=at+home+should...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>Google</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38215 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Type                                              Query  \\\n",
       "0       Searched for                                   ayurveda dataset   \n",
       "3            Visited  How Data Mining is useful in Ayurveda - Journa...   \n",
       "6            Visited  Graph Algorithms: Practical Examples in Apache...   \n",
       "9       Searched for  Graph Algorithms: Practical Examples in Apache...   \n",
       "12      Searched for                  graph analytics data science book   \n",
       "...              ...                                                ...   \n",
       "116052       Visited  http://www.stack.com/2013/11/11/at-home-should...   \n",
       "116055       Visited  http://ashotofadrenaline.net/20-hardest-should...   \n",
       "116058  Searched for                       no weight shoulder exercises   \n",
       "116061       Visited  http://www.mensfitness.com/training/workout-ro...   \n",
       "116064  Searched for               at home shoulder workouts no weights   \n",
       "\n",
       "                            Date  \\\n",
       "0      2020-09-19 05:26:40+00:00   \n",
       "3      2020-09-19 05:17:48+00:00   \n",
       "6      2020-09-09 03:11:32+00:00   \n",
       "9      2020-09-09 03:11:23+00:00   \n",
       "12     2020-09-09 03:10:53+00:00   \n",
       "...                          ...   \n",
       "116052 2015-08-17 01:21:25+00:00   \n",
       "116055 2015-08-17 01:15:51+00:00   \n",
       "116058 2015-08-17 01:15:40+00:00   \n",
       "116061 2015-08-17 01:11:06+00:00   \n",
       "116064 2015-08-17 01:11:02+00:00   \n",
       "\n",
       "                                                      URL  \\\n",
       "0        https://www.google.com/search?q=ayurveda+dataset   \n",
       "3       http://www.ayurvedjournal.com/JAHM_201623_01.p...   \n",
       "6       https://www.goodreads.com/book/show/42832585-g...   \n",
       "9       https://www.google.com/search?q=Graph+Algorith...   \n",
       "12      https://www.google.com/search?q=graph+analytic...   \n",
       "...                                                   ...   \n",
       "116052  http://www.stack.com/2013/11/11/at-home-should...   \n",
       "116055  http://ashotofadrenaline.net/20-hardest-should...   \n",
       "116058  https://www.google.com/search?q=no+weight+shou...   \n",
       "116061  http://www.mensfitness.com/training/workout-ro...   \n",
       "116064  https://www.google.com/search?q=at+home+should...   \n",
       "\n",
       "                                                 Location         Day  Mobile  \\\n",
       "0       https://www.google.com/maps/@40.723780,-73.971...  2020-09-19   False   \n",
       "3                                                    None  2020-09-19   False   \n",
       "6                                                    None  2020-09-09   False   \n",
       "9       https://www.google.com/maps/@40.723780,-73.971...  2020-09-09   False   \n",
       "12      https://www.google.com/maps/@40.723780,-73.971...  2020-09-09   False   \n",
       "...                                                   ...         ...     ...   \n",
       "116052                                               None  2015-08-17   False   \n",
       "116055                                               None  2015-08-17   False   \n",
       "116058                                               None  2015-08-17   False   \n",
       "116061                                               None  2015-08-17   False   \n",
       "116064                                               None  2015-08-17   False   \n",
       "\n",
       "                     Site                                               City  \n",
       "0                  Google  East River, New York County, New York, United ...  \n",
       "3          Ayurvedjournal                                               None  \n",
       "6               Goodreads                                               None  \n",
       "9                  Google  East River, New York County, New York, United ...  \n",
       "12                 Google  East River, New York County, New York, United ...  \n",
       "...                   ...                                                ...  \n",
       "116052              Stack                                               None  \n",
       "116055  Ashotofadrenaline                                               None  \n",
       "116058             Google                                               None  \n",
       "116061        Mensfitness                                               None  \n",
       "116064             Google                                               None  \n",
       "\n",
       "[38215 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "gl = Nominatim(user_agent=\"Google Data App v1.0\", timeout=10)\n",
    "\n",
    "class GenerateFeatures:\n",
    "    def __init__(self):\n",
    "        self.known_locations = {}\n",
    "        self.df = None\n",
    "        \n",
    "    def fit(self, df=None, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "        df['Day'] = df['Date'].dt.date\n",
    "        df['Mobile'] = self.on_mobile(df)\n",
    "        df['Site'] = df['URL'].apply(self.extract_site_name)\n",
    "    \n",
    "        # Converting coordiantes to addresses is very slow.\n",
    "        df['City'] = df['Location'].apply(lambda x: self.get_address_from_coords(x))\n",
    "        self.df = df\n",
    "        return df\n",
    "    \n",
    "    def extract_site_name(self, row):\n",
    "    \n",
    "        row = row.replace('//amp.', '').replace('.m.', '')\n",
    "\n",
    "        url_www = row.split('www.')\n",
    "        url_https = row.split('//')\n",
    "        url_mob = row.split('//m.')\n",
    "\n",
    "        if len(url_mob) > 1:\n",
    "            cleaned_row = url_mob[1].split('/')[0].split('.')[0]\n",
    "            return cleaned_row.title()\n",
    "        elif len(url_www) > 1:\n",
    "            cleaned_row = url_www[1].split('/')[0].split('.')[0]\n",
    "            return cleaned_row.title()\n",
    "        elif len(url_https) > 1:\n",
    "            cleaned_row = url_https[1].split('/')[0].split('.')[0]\n",
    "            return cleaned_row.title()\n",
    "        return row.title()\n",
    "\n",
    "    def on_mobile(self, row):\n",
    "        url_mob_formats = ['www.m.', 'https//m.', 'http://m.', '//amp.']\n",
    "\n",
    "        for elem in url_mob_formats:\n",
    "            if elem in row:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_address_from_coords(self, row):\n",
    "        \"\"\" Given a latitude & longitude --> return an approximate address. \"\"\"\n",
    "        if row:\n",
    "            # Split the row into latitude and longitude from the URL.\n",
    "            long, lat = row.split('@')[1].split(',')[0:2]\n",
    "            coordinates = str(round(float(long), 2)) + ' ' + str(round(float(lat), 2))\n",
    "            try:\n",
    "                # Skip API call is key exists in dictionary (more server friendly)\n",
    "                return self.known_locations[coordinates]\n",
    "            except KeyError:\n",
    "                city = gl.reverse(f\"{long}, {lat}\").address\n",
    "                self.known_locations[coordinates] = city\n",
    "                return city\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def save_file(self, file_name='google_data', file_type='csv'):\n",
    "        file = file_name + '.' + file_type.lower()\n",
    "        \n",
    "        if file_type.lower() == 'csv':\n",
    "            self.df.to_csv(file)\n",
    "        if file_type.lower() == \"xlsx\":\n",
    "            self.df.to_excel(file)\n",
    "        \n",
    "        print(f'Saved transformed dataset to: \"{file}\"')\n",
    "        \n",
    "df_new_feats = GenerateFeatures().transform(gd.df)\n",
    "df_new_feats.save_file(file_name=OUTPUT_FILE, file_type=OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Top Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA known not to be perfect for short-text topic modeling, but requires additionally library for GSDMM implementation of modified LDA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "class ModelData:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.model_name = None\n",
    "        self.my_punc = '[!\"$·%&\\'(”…#)—*+,-./:;<=>?“[\\\\]^_`{|}~•–@®]'\n",
    "        self.my_stopwords = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer(ignore_stopwords=False).stem\n",
    "        \n",
    "    def remove_urls(self, query):\n",
    "        \"\"\" Queries that are Google Search URLs should be filtered out. \"\"\"\n",
    "        if 'http' in query or 'www.' in query:\n",
    "            return None\n",
    "        else:\n",
    "            return query\n",
    "\n",
    "    def remove_punct(self, query, custom_punc=None):\n",
    "        \"\"\" Punctuation, especially contractions will harm data quality. Filter it out and return a list of token. \"\"\"\n",
    "        if custom_punc:\n",
    "            self.my_punc = custom_punc\n",
    "\n",
    "        tokens = []\n",
    "        \n",
    "        # Split query into words, check if each word contains punctuation. If so, remove and add to tokens list. Otherwise just append as-is.\n",
    "        for word in query.split():\n",
    "            word_clean = ''\n",
    "            for char in word:\n",
    "                if char not in self.my_punc:\n",
    "                    word_clean += char\n",
    "            tokens.append(word_clean)\n",
    "        return ' '.join(tokens)        \n",
    "\n",
    "    def remove_stopwords(self, query):\n",
    "        \"\"\" Remove the common stopwords from queries. Note: also removes negations (e.g. no, un, not) since they're stopwords. \"\"\"\n",
    "        tokens = word_tokenize(query)\n",
    "        tokens = [word for word in tokens\n",
    "                        if word not in self.my_stopwords] \n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def stem_query(self, query):\n",
    "        \"\"\" Returns the stem of each word in the query. \"\"\"\n",
    "        tokens = []\n",
    "        for word in query.split():\n",
    "            tokens.append(self.stemmer(word))\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def clean_string(self, query, bigrams=False):\n",
    "        \"\"\" Performs the actual query cleaning. \"\"\"\n",
    "        tokens = None\n",
    "        # Basic cleanup\n",
    "        query = query.lower()\n",
    "        query = self.remove_urls(query)\n",
    "        \n",
    "        # Check if query is still valid. Process for modeling if so.\n",
    "        if query:\n",
    "            query = self.remove_punct(query)\n",
    "            query = self.remove_stopwords(query)\n",
    "            query = self.stem_query(query)\n",
    "            tokens = query\n",
    "        \n",
    "        # Combine two elements to see in bigrams.\n",
    "        if bigrams:\n",
    "            tokens = tokens + [tokens[i] + '_' + tokens[i+1]\n",
    "                                    for i in range(len(tokens)-1)]\n",
    "        return tokens        \n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\" Transforms the entire Series. \"\"\"\n",
    "        self.df = df\n",
    "        self.df['Query Clean'] = df['Query'].apply(self.clean_string)\n",
    "        return self\n",
    "    \n",
    "    def drop_null_queries(self):\n",
    "        return self.df[self.df['Query Clean'].notna()]\n",
    "    \n",
    "    def _vectorize(self):\n",
    "        \"\"\" Convert each query string into a matrix of token counts. \"\"\"\n",
    "        df = self.drop_null_queries()\n",
    "        # Transforms text into vector\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "        print(f'Modeling on {len(df)} queries')\n",
    "        # Apply transformation\n",
    "        tf = vectorizer.fit_transform(df['Query Clean']).toarray()\n",
    "\n",
    "        # tf_feature_names gets the meaning of each column word.\n",
    "        tf_feature_names = vectorizer.get_feature_names()\n",
    "        return tf, tf_feature_names\n",
    "    \n",
    "    def _display_topics(self, model, feature_names, no_top_words):\n",
    "        \"\"\" Format the results of vectorization for display in table. Code from: https://bit.ly/3lUIWJZ \"\"\"\n",
    "        topic_dict = {}\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "            topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        return pd.DataFrame(topic_dict)\n",
    "    \n",
    "    def generate_topics(self, model_name=None, num_topics=None, num_top_words=None):\n",
    "        \"\"\" Takes a model type, # of topics, # of words per topic, and returns the topics in a table. \"\"\"\n",
    "        if not num_topics:\n",
    "            num_topics = 10\n",
    "        if not num_top_words:\n",
    "            num_top_words = 5\n",
    "            \n",
    "        tf, tf_feature_names = self._vectorize()\n",
    "        \n",
    "        # Assume either LDA or NMF parameter is passed.\n",
    "        if model_name == 'LDA':\n",
    "            lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "            lda.fit(tf)\n",
    "            return self._display_topics(lda, tf_feature_names, num_top_words)\n",
    "        if model_name == 'NMF':\n",
    "            nmf = NMF(n_components=num_topics, random_state=0, alpha=.1, l1_ratio=.5)\n",
    "            nmf.fit(tf)\n",
    "            return self._display_topics(nmf, tf_feature_names, num_top_words)\n",
    "        else:\n",
    "            print('Unsupported model type')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ModelData at 0x7fe8c6f71d00>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelData()\n",
    "model.fit(gd.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling on 30223 queries\n"
     ]
    }
   ],
   "source": [
    "topics_lda = model.generate_topics(model_name='LDA', num_topics=10, num_top_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling on 30223 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "topics_nmf = model.generate_topics(model_name='NMF', num_topics=10, num_top_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0 words</th>\n",
       "      <th>Topic 0 weights</th>\n",
       "      <th>Topic 1 words</th>\n",
       "      <th>Topic 1 weights</th>\n",
       "      <th>Topic 2 words</th>\n",
       "      <th>Topic 2 weights</th>\n",
       "      <th>Topic 3 words</th>\n",
       "      <th>Topic 3 weights</th>\n",
       "      <th>Topic 4 words</th>\n",
       "      <th>Topic 4 weights</th>\n",
       "      <th>Topic 5 words</th>\n",
       "      <th>Topic 5 weights</th>\n",
       "      <th>Topic 6 words</th>\n",
       "      <th>Topic 6 weights</th>\n",
       "      <th>Topic 7 words</th>\n",
       "      <th>Topic 7 weights</th>\n",
       "      <th>Topic 8 words</th>\n",
       "      <th>Topic 8 weights</th>\n",
       "      <th>Topic 9 words</th>\n",
       "      <th>Topic 9 weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>python</td>\n",
       "      <td>8.9</td>\n",
       "      <td>ben</td>\n",
       "      <td>5.5</td>\n",
       "      <td>mac</td>\n",
       "      <td>8.5</td>\n",
       "      <td>best</td>\n",
       "      <td>8.2</td>\n",
       "      <td>data</td>\n",
       "      <td>7.0</td>\n",
       "      <td>stack</td>\n",
       "      <td>6.1</td>\n",
       "      <td>colleg</td>\n",
       "      <td>6.0</td>\n",
       "      <td>linux</td>\n",
       "      <td>5.5</td>\n",
       "      <td>reddit</td>\n",
       "      <td>7.6</td>\n",
       "      <td>download</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>get</td>\n",
       "      <td>1.0</td>\n",
       "      <td>greenfield</td>\n",
       "      <td>5.4</td>\n",
       "      <td>torrent</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.9</td>\n",
       "      <td>scienc</td>\n",
       "      <td>3.6</td>\n",
       "      <td>overflow</td>\n",
       "      <td>5.1</td>\n",
       "      <td>health</td>\n",
       "      <td>2.1</td>\n",
       "      <td>kali</td>\n",
       "      <td>4.3</td>\n",
       "      <td>io</td>\n",
       "      <td>1.3</td>\n",
       "      <td>torrent</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>string</td>\n",
       "      <td>0.8</td>\n",
       "      <td>fit</td>\n",
       "      <td>2.0</td>\n",
       "      <td>x</td>\n",
       "      <td>0.8</td>\n",
       "      <td>onlin</td>\n",
       "      <td>0.7</td>\n",
       "      <td>scientist</td>\n",
       "      <td>0.7</td>\n",
       "      <td>use</td>\n",
       "      <td>1.3</td>\n",
       "      <td>claremont</td>\n",
       "      <td>1.7</td>\n",
       "      <td>forum</td>\n",
       "      <td>1.8</td>\n",
       "      <td>jailbreak</td>\n",
       "      <td>0.7</td>\n",
       "      <td>mp3</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>list</td>\n",
       "      <td>0.7</td>\n",
       "      <td>workout</td>\n",
       "      <td>0.4</td>\n",
       "      <td>os</td>\n",
       "      <td>0.8</td>\n",
       "      <td>cours</td>\n",
       "      <td>0.6</td>\n",
       "      <td>learn</td>\n",
       "      <td>0.4</td>\n",
       "      <td>get</td>\n",
       "      <td>1.1</td>\n",
       "      <td>pitzer</td>\n",
       "      <td>1.1</td>\n",
       "      <td>instal</td>\n",
       "      <td>1.1</td>\n",
       "      <td>vs</td>\n",
       "      <td>0.4</td>\n",
       "      <td>io</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file</td>\n",
       "      <td>0.6</td>\n",
       "      <td>healthi</td>\n",
       "      <td>0.3</td>\n",
       "      <td>app</td>\n",
       "      <td>0.7</td>\n",
       "      <td>app</td>\n",
       "      <td>0.6</td>\n",
       "      <td>machin</td>\n",
       "      <td>0.4</td>\n",
       "      <td>file</td>\n",
       "      <td>0.8</td>\n",
       "      <td>pomona</td>\n",
       "      <td>0.9</td>\n",
       "      <td>raspberri</td>\n",
       "      <td>0.7</td>\n",
       "      <td>coupon</td>\n",
       "      <td>0.4</td>\n",
       "      <td>free</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic 0 words Topic 0 weights Topic 1 words Topic 1 weights Topic 2 words  \\\n",
       "0        python             8.9           ben             5.5           mac   \n",
       "1           get             1.0    greenfield             5.4       torrent   \n",
       "2        string             0.8           fit             2.0             x   \n",
       "3          list             0.7       workout             0.4            os   \n",
       "4          file             0.6       healthi             0.3           app   \n",
       "\n",
       "  Topic 2 weights Topic 3 words Topic 3 weights Topic 4 words Topic 4 weights  \\\n",
       "0             8.5          best             8.2          data             7.0   \n",
       "1             0.8          2020             0.9        scienc             3.6   \n",
       "2             0.8         onlin             0.7     scientist             0.7   \n",
       "3             0.8         cours             0.6         learn             0.4   \n",
       "4             0.7           app             0.6        machin             0.4   \n",
       "\n",
       "  Topic 5 words Topic 5 weights Topic 6 words Topic 6 weights Topic 7 words  \\\n",
       "0         stack             6.1        colleg             6.0         linux   \n",
       "1      overflow             5.1        health             2.1          kali   \n",
       "2           use             1.3     claremont             1.7         forum   \n",
       "3           get             1.1        pitzer             1.1        instal   \n",
       "4          file             0.8        pomona             0.9     raspberri   \n",
       "\n",
       "  Topic 7 weights Topic 8 words Topic 8 weights Topic 9 words Topic 9 weights  \n",
       "0             5.5        reddit             7.6      download             7.1  \n",
       "1             4.3            io             1.3       torrent             1.8  \n",
       "2             1.8     jailbreak             0.7           mp3             1.7  \n",
       "3             1.1            vs             0.4            io             1.3  \n",
       "4             0.7        coupon             0.4          free             1.2  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_nmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Obselete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA known not to be perfect for short-text topic modeling, but requires additionally library for GSDMM implementation of modified LDA\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "my_punctuation = '[!\"$·%&\\'(”…#)—*+,-./:;<=>?“[\\\\]^_`{|}~•–@®]'\n",
    "\n",
    "def remove_urls(query):\n",
    "    \"\"\" Queries that are Google Search URLs should be filtered out. \"\"\"\n",
    "    if 'http' in query or 'www.' in query:\n",
    "        return None\n",
    "    else:\n",
    "        return query\n",
    "\n",
    "def remove_punct(query, custom_punc=None):\n",
    "    \"\"\" Punctuation, especially contractions will harm data quality. Filter it out and return a list of token. \"\"\"\n",
    "    if custom_punc:\n",
    "        my_punctuation = custom_punc\n",
    "    else:\n",
    "        my_punctuation = '[!\"$·%&\\'(”…#)—*+,-./:;<=>?“[\\\\]^_`{|}~•–@®]'\n",
    "\n",
    "    tokens = []\n",
    "    \n",
    "    # Split query into words, check if each word contains punctuation. If so, remove and add to tokens list. Otherwise just append as-is.\n",
    "    for word in query.split():\n",
    "        word_clean = ''\n",
    "        for char in word:\n",
    "            if char not in my_punctuation:\n",
    "                word_clean += char\n",
    "        tokens.append(word_clean)\n",
    "    \n",
    "    return ' '.join(tokens)        \n",
    "    \n",
    "def remove_stopwords(query):\n",
    "    \"\"\" Remove the common stopwords from queries. Note: also removes negations (e.g. no, un, not) since they're stopwords. \"\"\"\n",
    "    tokens = word_tokenize(query)\n",
    "    tokens = [word for word in tokens\n",
    "                    if word not in my_stopwords] \n",
    "    return \" \".join(tokens)\n",
    "    \n",
    "def stem_query(query):\n",
    "    tokens = []\n",
    "    for word in query:\n",
    "        tokens.append(word_rooter(word))\n",
    "    return \" \".join(tokens)\n",
    "    \n",
    "def clean_search(query, bigrams=False):\n",
    "    # Basic cleanup\n",
    "    query = query.lower()\n",
    "    query = remove_urls(query)\n",
    "    \n",
    "    # Check if it's still valid. Process for modeling if so.\n",
    "    if query:\n",
    "        query = remove_punct(query)\n",
    "        query = remove_stopwords(query)\n",
    "        tokens = stem_query(query)\n",
    "    \n",
    "    # Combine two elements to see in bigrams.\n",
    "    if bigrams:\n",
    "        tokens = tokens + [tokens[i] + '_' + tokens[i+1]\n",
    "                                for i in range(len(tokens)-1)]\n",
    "    \n",
    "    return query\n",
    "\n",
    "df = gd.df\n",
    "df['Clean Query'] = df['Query'].apply(clean_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://ourcodingclub.github.io/tutorials/topic-modelling-python/#who_what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# the vectorizer object will be used to transform text to vector form\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "# apply transformation\n",
    "tf = vectorizer.fit_transform(search_query_df['Clean Query']).toarray()\n",
    "\n",
    "# tf_feature_names tells us what word each column in the matric represents\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "# tf_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topic Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(random_state=0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "number_of_topics = 10\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
    "model.fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Top Google Searches (LDA):\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModelData' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-6806af47fcc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'My Top Google Searches (LDA):'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-6768979d7b8e>\u001b[0m in \u001b[0;36mdisplay_topics\u001b[0;34m(model, feature_names, no_top_words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtopic_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n\u001b[1;32m      5\u001b[0m                         for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelData' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "no_top_words = 5\n",
    "\n",
    "print('My Top Google Searches (LDA):')\n",
    "display_topics(model, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.1, l1_ratio=0.5, n_components=10, random_state=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=number_of_topics, random_state=0, alpha=.1, l1_ratio=.5)\n",
    "\n",
    "nmf.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Top Google Searches (NMF):\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-eab004423877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'My Top Google Searches (NMF):'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnmf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-6768979d7b8e>\u001b[0m in \u001b[0;36mdisplay_topics\u001b[0;34m(model, feature_names, no_top_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtopic_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n\u001b[0m\u001b[1;32m      5\u001b[0m                         for i in topic.argsort()[:-no_top_words - 1:-1]]\n\u001b[1;32m      6\u001b[0m         topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
      "\u001b[0;32m<ipython-input-71-6768979d7b8e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtopic_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n\u001b[0m\u001b[1;32m      5\u001b[0m                         for i in topic.argsort()[:-no_top_words - 1:-1]]\n\u001b[1;32m      6\u001b[0m         topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('My Top Google Searches (NMF):')\n",
    "display_topics(nmf, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Query</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Location</th>\n",
       "      <th>Day</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>City</th>\n",
       "      <th>Site</th>\n",
       "      <th>Clean Query</th>\n",
       "      <th>Query Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>ayurveda dataset</td>\n",
       "      <td>2020-09-19 05:26:40+00:00</td>\n",
       "      <td>https://www.google.com/search?q=ayurveda+dataset</td>\n",
       "      <td>https://www.google.com/maps/@40.723780,-73.971...</td>\n",
       "      <td>2020-09-19</td>\n",
       "      <td>False</td>\n",
       "      <td>East River, New York County, New York, United ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>ayurveda dataset</td>\n",
       "      <td>ayurveda dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Visited</td>\n",
       "      <td>How Data Mining is useful in Ayurveda - Journa...</td>\n",
       "      <td>2020-09-19 05:17:48+00:00</td>\n",
       "      <td>http://www.ayurvedjournal.com/JAHM_201623_01.p...</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-09-19</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Ayurvedjournal</td>\n",
       "      <td>data mining useful ayurveda journal ayurvedic</td>\n",
       "      <td>how data mining is useful in ayurveda  journal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Visited</td>\n",
       "      <td>Graph Algorithms: Practical Examples in Apache...</td>\n",
       "      <td>2020-09-09 03:11:32+00:00</td>\n",
       "      <td>https://www.goodreads.com/book/show/42832585-g...</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Goodreads</td>\n",
       "      <td>graph algorithms practical examples apache spa...</td>\n",
       "      <td>graph algorithms practical examples in apache ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>Graph Algorithms: Practical Examples in Apache...</td>\n",
       "      <td>2020-09-09 03:11:23+00:00</td>\n",
       "      <td>https://www.google.com/search?q=Graph+Algorith...</td>\n",
       "      <td>https://www.google.com/maps/@40.723780,-73.971...</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>False</td>\n",
       "      <td>East River, New York County, New York, United ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>graph algorithms practical examples apache spa...</td>\n",
       "      <td>graph algorithms practical examples in apache ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>graph analytics data science book</td>\n",
       "      <td>2020-09-09 03:10:53+00:00</td>\n",
       "      <td>https://www.google.com/search?q=graph+analytic...</td>\n",
       "      <td>https://www.google.com/maps/@40.723780,-73.971...</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>False</td>\n",
       "      <td>East River, New York County, New York, United ...</td>\n",
       "      <td>Google</td>\n",
       "      <td>graph analytics data science book</td>\n",
       "      <td>graph analytics data science book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116052</th>\n",
       "      <td>Visited</td>\n",
       "      <td>http://www.stack.com/2013/11/11/at-home-should...</td>\n",
       "      <td>2015-08-17 01:21:25+00:00</td>\n",
       "      <td>http://www.stack.com/2013/11/11/at-home-should...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Stack</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116055</th>\n",
       "      <td>Visited</td>\n",
       "      <td>http://ashotofadrenaline.net/20-hardest-should...</td>\n",
       "      <td>2015-08-17 01:15:51+00:00</td>\n",
       "      <td>http://ashotofadrenaline.net/20-hardest-should...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Ashotofadrenaline</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116058</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>no weight shoulder exercises</td>\n",
       "      <td>2015-08-17 01:15:40+00:00</td>\n",
       "      <td>https://www.google.com/search?q=no+weight+shou...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Google</td>\n",
       "      <td>weight shoulder exercises</td>\n",
       "      <td>no weight shoulder exercises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116061</th>\n",
       "      <td>Visited</td>\n",
       "      <td>http://www.mensfitness.com/training/workout-ro...</td>\n",
       "      <td>2015-08-17 01:11:06+00:00</td>\n",
       "      <td>http://www.mensfitness.com/training/workout-ro...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Mensfitness</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116064</th>\n",
       "      <td>Searched for</td>\n",
       "      <td>at home shoulder workouts no weights</td>\n",
       "      <td>2015-08-17 01:11:02+00:00</td>\n",
       "      <td>https://www.google.com/search?q=at+home+should...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Google</td>\n",
       "      <td>home shoulder workouts weights</td>\n",
       "      <td>at home shoulder workouts no weights</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38215 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Type                                              Query  \\\n",
       "0       Searched for                                   ayurveda dataset   \n",
       "3            Visited  How Data Mining is useful in Ayurveda - Journa...   \n",
       "6            Visited  Graph Algorithms: Practical Examples in Apache...   \n",
       "9       Searched for  Graph Algorithms: Practical Examples in Apache...   \n",
       "12      Searched for                  graph analytics data science book   \n",
       "...              ...                                                ...   \n",
       "116052       Visited  http://www.stack.com/2013/11/11/at-home-should...   \n",
       "116055       Visited  http://ashotofadrenaline.net/20-hardest-should...   \n",
       "116058  Searched for                       no weight shoulder exercises   \n",
       "116061       Visited  http://www.mensfitness.com/training/workout-ro...   \n",
       "116064  Searched for               at home shoulder workouts no weights   \n",
       "\n",
       "                            Date  \\\n",
       "0      2020-09-19 05:26:40+00:00   \n",
       "3      2020-09-19 05:17:48+00:00   \n",
       "6      2020-09-09 03:11:32+00:00   \n",
       "9      2020-09-09 03:11:23+00:00   \n",
       "12     2020-09-09 03:10:53+00:00   \n",
       "...                          ...   \n",
       "116052 2015-08-17 01:21:25+00:00   \n",
       "116055 2015-08-17 01:15:51+00:00   \n",
       "116058 2015-08-17 01:15:40+00:00   \n",
       "116061 2015-08-17 01:11:06+00:00   \n",
       "116064 2015-08-17 01:11:02+00:00   \n",
       "\n",
       "                                                      URL  \\\n",
       "0        https://www.google.com/search?q=ayurveda+dataset   \n",
       "3       http://www.ayurvedjournal.com/JAHM_201623_01.p...   \n",
       "6       https://www.goodreads.com/book/show/42832585-g...   \n",
       "9       https://www.google.com/search?q=Graph+Algorith...   \n",
       "12      https://www.google.com/search?q=graph+analytic...   \n",
       "...                                                   ...   \n",
       "116052  http://www.stack.com/2013/11/11/at-home-should...   \n",
       "116055  http://ashotofadrenaline.net/20-hardest-should...   \n",
       "116058  https://www.google.com/search?q=no+weight+shou...   \n",
       "116061  http://www.mensfitness.com/training/workout-ro...   \n",
       "116064  https://www.google.com/search?q=at+home+should...   \n",
       "\n",
       "                                                 Location         Day  Mobile  \\\n",
       "0       https://www.google.com/maps/@40.723780,-73.971...  2020-09-19   False   \n",
       "3                                                    None  2020-09-19   False   \n",
       "6                                                    None  2020-09-09   False   \n",
       "9       https://www.google.com/maps/@40.723780,-73.971...  2020-09-09   False   \n",
       "12      https://www.google.com/maps/@40.723780,-73.971...  2020-09-09   False   \n",
       "...                                                   ...         ...     ...   \n",
       "116052                                               None  2015-08-17   False   \n",
       "116055                                               None  2015-08-17   False   \n",
       "116058                                               None  2015-08-17   False   \n",
       "116061                                               None  2015-08-17   False   \n",
       "116064                                               None  2015-08-17   False   \n",
       "\n",
       "                                                     City               Site  \\\n",
       "0       East River, New York County, New York, United ...             Google   \n",
       "3                                                    None     Ayurvedjournal   \n",
       "6                                                    None          Goodreads   \n",
       "9       East River, New York County, New York, United ...             Google   \n",
       "12      East River, New York County, New York, United ...             Google   \n",
       "...                                                   ...                ...   \n",
       "116052                                               None              Stack   \n",
       "116055                                               None  Ashotofadrenaline   \n",
       "116058                                               None             Google   \n",
       "116061                                               None        Mensfitness   \n",
       "116064                                               None             Google   \n",
       "\n",
       "                                              Clean Query  \\\n",
       "0                                        ayurveda dataset   \n",
       "3           data mining useful ayurveda journal ayurvedic   \n",
       "6       graph algorithms practical examples apache spa...   \n",
       "9       graph algorithms practical examples apache spa...   \n",
       "12                      graph analytics data science book   \n",
       "...                                                   ...   \n",
       "116052                                               None   \n",
       "116055                                               None   \n",
       "116058                          weight shoulder exercises   \n",
       "116061                                               None   \n",
       "116064                     home shoulder workouts weights   \n",
       "\n",
       "                                              Query Clean  \n",
       "0                                        ayurveda dataset  \n",
       "3       how data mining is useful in ayurveda  journal...  \n",
       "6       graph algorithms practical examples in apache ...  \n",
       "9       graph algorithms practical examples in apache ...  \n",
       "12                      graph analytics data science book  \n",
       "...                                                   ...  \n",
       "116052                                               None  \n",
       "116055                                               None  \n",
       "116058                       no weight shoulder exercises  \n",
       "116061                                               None  \n",
       "116064               at home shoulder workouts no weights  \n",
       "\n",
       "[38215 rows x 11 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
