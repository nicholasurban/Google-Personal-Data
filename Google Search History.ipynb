{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Information From Personal Google Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Only the \"My Activity\" directory is supported\n",
    "path = '../Datasets/Google-20-09/My Activity/Search/MyActivity.html'\n",
    "\n",
    "OUTPUT_FILE = 'google_data'\n",
    "OUTPUT_TYPE = 'csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Google Data HTML To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: [DONE]\n",
      "Parsing HTML file: [DONE]\n",
      "Successfully processed HTML element: 0\n",
      "Successfully processed HTML element: 5000\n",
      "Successfully processed HTML element: 10000\n",
      "Successfully processed HTML element: 15000\n",
      "Successfully processed HTML element: 20000\n",
      "Successfully processed HTML element: 25000\n",
      "Successfully processed HTML element: 30000\n",
      "Successfully processed HTML element: 35000\n",
      "Successfully processed HTML element: 40000\n",
      "Successfully processed HTML element: 45000\n",
      "Successfully processed HTML element: 50000\n",
      "Successfully processed HTML element: 55000\n",
      "Successfully processed HTML element: 60000\n",
      "Successfully processed HTML element: 65000\n",
      "Successfully processed HTML element: 70000\n",
      "Successfully processed HTML element: 75000\n",
      "Successfully processed HTML element: 80000\n",
      "Successfully processed HTML element: 85000\n",
      "Successfully processed HTML element: 90000\n",
      "Successfully processed HTML element: 95000\n",
      "Successfully processed HTML element: 100000\n",
      "Successfully processed HTML element: 105000\n",
      "Successfully processed HTML element: 110000\n",
      "Successfully processed HTML element: 115000\n",
      "Finished processing rows\n",
      "Saved 38215 rows to: \"google_data.csv\"\n"
     ]
    }
   ],
   "source": [
    "class ProcessGoogleData:\n",
    "    ''' Handles Google Data '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.html = None\n",
    "        self.cells = None\n",
    "        self.df = None\n",
    "        self.file_name = None\n",
    " \n",
    "    def read_file(self, path):\n",
    "        \"\"\" Given a file path, read it into this object. \"\"\"\n",
    "        print('Reading file: ', end='')\n",
    "        with open(path, 'r') as f:\n",
    "            self.html = f.read()    \n",
    "        print('[DONE]')\n",
    "            \n",
    "    def parse(self):\n",
    "        \"\"\" Input HTML data and parse through cells. \"\"\"\n",
    "        print('Parsing HTML file: ', end='')\n",
    "        soup = BeautifulSoup(self.html, 'lxml')\n",
    "        self.cells = soup.select('.content-cell')\n",
    "        print('[DONE]')\n",
    "    \n",
    "    def _process_row(self, row):\n",
    "        a = row.find('a')             \n",
    "        row_dict = {}\n",
    "        \n",
    "        try:\n",
    "            search_query = str(a.text)\n",
    "            search_type = str(a.parent.contents[0]).strip()\n",
    "            location = self._get_location(a)\n",
    "\n",
    "            # Handle different string format for Visited pages\n",
    "            if search_type == 'Visited':\n",
    "                site_url = a.get('href').split('?q=')[1]\n",
    "            else:\n",
    "                site_url = a.get('href')\n",
    "\n",
    "            search_date = pd.to_datetime(a.parent.contents[3])\n",
    "\n",
    "            # Build the dictionary\n",
    "            row_dict = {\n",
    "                'Type':search_type,\n",
    "                'Query': search_query,\n",
    "                'Date': search_date,\n",
    "                'URL': site_url,\n",
    "                'Location': location\n",
    "            }\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        return row_dict\n",
    "    \n",
    "    def _get_location(self, url):\n",
    "        \"\"\" Given a list of cells, find the location URL. \"\"\"\n",
    "        href_next = url.find_next('a').get('href')\n",
    "        \n",
    "        if 'google.com/maps/@?api=1' in href_next:\n",
    "            coordinates = href_next.split('center=')[1].split('&zoom')[0]\n",
    "            full_maps_url = 'https://www.google.com/maps/@' + coordinates + ',17z' \n",
    "            return full_maps_url\n",
    "        return None\n",
    "    \n",
    "    def create_table(self):\n",
    "        \"\"\" Takes a BS4 element filter, and appends each Google data cell into a dataframe row. \"\"\"        \n",
    "        meta_dict = {}\n",
    "        \n",
    "        cell_list = self.cells\n",
    "        \n",
    "        for i, elem in enumerate(cell_list):\n",
    "            row = self._process_row(elem)\n",
    "            meta_dict[i] = row\n",
    "\n",
    "            # Files are large, update status every 5000 elements\n",
    "            if (i % 5000 == 0):\n",
    "                print(f'Successfully processed HTML element: {i}')\n",
    "\n",
    "        df = pd.DataFrame.from_dict(data=meta_dict, orient='index')\n",
    "        print('Finished processing rows')\n",
    "        self.df = df\n",
    "        \n",
    "    def save_file(self, file_name='google_data', file_type='csv'):\n",
    "        \"\"\" Outputs the file in either of two formats before engineering more features. \"\"\"\n",
    "        file = file_name + '.' + file_type.lower()\n",
    "        self.file_name = file\n",
    "        \n",
    "        if file_type.lower() == 'csv':\n",
    "            self.df.to_csv(file)\n",
    "        if file_type.lower() == \"xlsx\":\n",
    "            self.df.to_excel(file)\n",
    "        \n",
    "        print(f'Saved {self.df.shape[0]} rows to: \"{file}\"')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    gd = ProcessGoogleData()\n",
    "    gd.read_file(path)\n",
    "    gd.parse()\n",
    "    gd.create_table()\n",
    "    \n",
    "    # Save to CSV\n",
    "    gd.save_file(file_name=OUTPUT_FILE, file_type=OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build New Features From Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformed dataset to: \"google_data.csv\"\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "gl = Nominatim(user_agent=\"Google Data App v1.0\", timeout=10)\n",
    "\n",
    "class GenerateFeatures:\n",
    "    def __init__(self):\n",
    "        self.known_locations = {}\n",
    "        self.df = None\n",
    "        \n",
    "    def fit(self, df=None, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\" Create the new feature transformations. \"\"\"\n",
    "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "        df['Day'] = df['Date'].dt.date\n",
    "        df['Mobile'] = self.on_mobile(df)\n",
    "        df['Site'] = df['URL'].apply(self.extract_site_name)\n",
    "    \n",
    "        # Converting coordiantes to addresses is very slow.\n",
    "        print('Converting GPS coordinates to address. Please wait.')\n",
    "        df['City'] = df['Location'].apply(lambda x: self.get_address_from_coords(x))\n",
    "        self.df = df\n",
    "        return self\n",
    "    \n",
    "    def extract_site_name(self, row):\n",
    "    \n",
    "        row = row.replace('//amp.', '').replace('.m.', '')\n",
    "\n",
    "        url_www = row.split('www.')\n",
    "        url_https = row.split('//')\n",
    "        url_mob = row.split('//m.')\n",
    "\n",
    "        if len(url_mob) > 1:\n",
    "            cleaned_row = url_mob[1].split('/')[0].split('.')[0]\n",
    "            return cleaned_row.title()\n",
    "        elif len(url_www) > 1:\n",
    "            cleaned_row = url_www[1].split('/')[0].split('.')[0]\n",
    "            return cleaned_row.title()\n",
    "        elif len(url_https) > 1:\n",
    "            cleaned_row = url_https[1].split('/')[0].split('.')[0]\n",
    "            return cleaned_row.title()\n",
    "        return row.title()\n",
    "\n",
    "    def on_mobile(self, row):\n",
    "        url_mob_formats = ['www.m.', 'https//m.', 'http://m.', '//amp.']\n",
    "\n",
    "        for elem in url_mob_formats:\n",
    "            if elem in row:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_address_from_coords(self, row):\n",
    "        \"\"\" Given a latitude & longitude --> return an approximate address. \"\"\"\n",
    "        if row:\n",
    "            # Split the row into latitude and longitude from the URL.\n",
    "            long, lat = row.split('@')[1].split(',')[0:2]\n",
    "            coordinates = str(round(float(long), 2)) + ' ' + str(round(float(lat), 2))\n",
    "            try:\n",
    "                # Skip API call is key exists in dictionary (more server friendly)\n",
    "                return self.known_locations[coordinates]\n",
    "            except KeyError:\n",
    "                city = gl.reverse(f\"{long}, {lat}\").address\n",
    "                self.known_locations[coordinates] = city\n",
    "                return city\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def save_file(self, file_name='google_data', file_type='csv'):\n",
    "        file = file_name + '.' + file_type.lower()\n",
    "        \n",
    "        if file_type.lower() == 'csv':\n",
    "            self.df.to_csv(file)\n",
    "        if file_type.lower() == \"xlsx\":\n",
    "            self.df.to_excel(file)\n",
    "        \n",
    "        print(f'Saved transformed dataset to: \"{file}\"')\n",
    "        \n",
    "df_new_feats = GenerateFeatures().transform(gd.df)\n",
    "df_new_feats.save_file(file_name=OUTPUT_FILE, file_type=OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Top Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "class ModelData:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.model_name = None\n",
    "        self.my_punc = '[!\"$·%&\\'(”…#)—*+,-./:;<=>?“[\\\\]^_`{|}~•–@®]'\n",
    "        self.my_stopwords = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer(ignore_stopwords=False).stem\n",
    "        \n",
    "    def remove_urls(self, query):\n",
    "        \"\"\" Queries that are Google Search URLs should be filtered out. \"\"\"\n",
    "        if 'http' in query or 'www.' in query:\n",
    "            return None\n",
    "        else:\n",
    "            return query\n",
    "\n",
    "    def remove_punct(self, query, custom_punc=None):\n",
    "        \"\"\" Punctuation, especially contractions will harm data quality. Filter it out and return a list of token. \"\"\"\n",
    "        if custom_punc:\n",
    "            self.my_punc = custom_punc\n",
    "\n",
    "        tokens = []\n",
    "        \n",
    "        # Split query into words, check if each word contains punctuation. If so, remove and add to tokens list. Otherwise just append as-is.\n",
    "        for word in query.split():\n",
    "            word_clean = ''\n",
    "            for char in word:\n",
    "                if char not in self.my_punc:\n",
    "                    word_clean += char\n",
    "            tokens.append(word_clean)\n",
    "        return ' '.join(tokens)        \n",
    "\n",
    "    def remove_stopwords(self, query):\n",
    "        \"\"\" Remove the common stopwords from queries. Note: also removes negations (e.g. no, un, not) since they're stopwords. \"\"\"\n",
    "        tokens = word_tokenize(query)\n",
    "        tokens = [word for word in tokens\n",
    "                        if word not in self.my_stopwords] \n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def stem_query(self, query):\n",
    "        \"\"\" Returns the stem of each word in the query. \"\"\"\n",
    "        tokens = []\n",
    "        for word in query.split():\n",
    "            tokens.append(self.stemmer(word))\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def clean_string(self, query, bigrams=False):\n",
    "        \"\"\" Performs the actual query cleaning. \"\"\"\n",
    "        tokens = None\n",
    "        # Basic cleanup\n",
    "        query = query.lower()\n",
    "        query = self.remove_urls(query)\n",
    "        \n",
    "        # Check if query is still valid. Process for modeling if so.\n",
    "        if query:\n",
    "            query = self.remove_punct(query)\n",
    "            query = self.remove_stopwords(query)\n",
    "            query = self.stem_query(query)\n",
    "            tokens = query\n",
    "        \n",
    "        # Combine two elements to see in bigrams.\n",
    "        if bigrams:\n",
    "            tokens = tokens + [tokens[i] + '_' + tokens[i+1]\n",
    "                                    for i in range(len(tokens)-1)]\n",
    "        return tokens        \n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\" Transforms the entire Series. \"\"\"\n",
    "        self.df = df\n",
    "        self.df['Query Clean'] = df['Query'].apply(self.clean_string)\n",
    "        return self\n",
    "    \n",
    "    def drop_null_queries(self):\n",
    "        return self.df[self.df['Query Clean'].notna()]\n",
    "    \n",
    "    def _vectorize(self):\n",
    "        \"\"\" Convert each query string into a matrix of token counts. \"\"\"\n",
    "        df = self.drop_null_queries()\n",
    "        # Transforms text into vector\n",
    "        vectorizer = CountVectorizer(max_df=0.9, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "        print(f'Modeling on {len(df)} valid queries')\n",
    "        # Apply transformation\n",
    "        tf = vectorizer.fit_transform(df['Query Clean']).toarray()\n",
    "\n",
    "        # tf_feature_names gets the meaning of each column word.\n",
    "        tf_feature_names = vectorizer.get_feature_names()\n",
    "        return tf, tf_feature_names\n",
    "    \n",
    "    def _display_topics(self, model, feature_names, no_top_words):\n",
    "        \"\"\" Format the results of vectorization for display in table. Code from: https://bit.ly/3lUIWJZ \"\"\"\n",
    "        topic_dict = {}\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "            topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        return pd.DataFrame(topic_dict)\n",
    "    \n",
    "    def generate_topics(self, model_name=None, num_topics=None, num_top_words=None):\n",
    "        \"\"\" Takes a model type, # of topics, # of words per topic, and returns the topics in a table. \"\"\"\n",
    "        if not num_topics:\n",
    "            num_topics = 10\n",
    "        if not num_top_words:\n",
    "            num_top_words = 5\n",
    "            \n",
    "        tf, tf_feature_names = self._vectorize()\n",
    "        \n",
    "        # Assume either LDA or NMF parameter is passed.\n",
    "        if model_name == 'LDA':\n",
    "            # LDA not great for short-text topic modeling. GSDMM modified LDA is better.\n",
    "            lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "            lda.fit(tf)\n",
    "            return self._display_topics(lda, tf_feature_names, num_top_words)\n",
    "        if model_name == 'NMF':\n",
    "            nmf = NMF(n_components=num_topics, alpha=.1, l1_ratio=.5)\n",
    "            nmf.fit(tf)\n",
    "            return self._display_topics(nmf, tf_feature_names, num_top_words)\n",
    "        else:\n",
    "            print('Unsupported model type')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ModelData at 0x7ff5b0aebf70>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelData()\n",
    "model.fit(gd.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling on 30223 valid queries\n"
     ]
    }
   ],
   "source": [
    "topics_lda = model.generate_topics(model_name='LDA', num_topics=10, num_top_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling on 1354 queries\n"
     ]
    }
   ],
   "source": [
    "topics_nmf = model.generate_topics(model_name='NMF', num_topics=10, num_top_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topics_nmf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
